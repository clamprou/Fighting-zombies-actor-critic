{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d63b5",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652d398",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import agent_class as agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a8abf93",
   "metadata": {},
   "source": [
    "# Initialize environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72227848",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# We first create the environment on which we will later train the agent\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# We need to know the dimensionality of the state space, as well as how many\n",
    "# actions are possible\n",
    "N_actions = env.action_space.n\n",
    "observation, info = env.reset()\n",
    "N_state = len(observation)\n",
    "\n",
    "print('dimension of state space =',N_state)\n",
    "print('number of actions =',N_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582229b0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# We create an instance of the agent class. \n",
    "# At initialization, we need to provide \n",
    "# - the dimensionality of the state space, as well as \n",
    "# - the number of possible actions\n",
    "\n",
    "parameters = {'N_state':N_state, 'N_actions':N_actions}\n",
    "\n",
    "my_agent = agent.dqn(parameters=parameters)\n",
    "# to train via the actor-critic algorithm, use this line:\n",
    "# my_agent = agent.actor_critic(parameters=parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d20d0162",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056479b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# We train the agent on the LunarLander-v2 environment.\n",
    "# Setting verbose=True allows us to follow the progress of the training\n",
    "\n",
    "training_results = my_agent.train(environment=env,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5a432",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# the method my_agent.train() from the previous cell returns a dictionary\n",
    "# with training stats, namely:\n",
    "# - duration of each episode during training,\n",
    "# - return of each episode during training\n",
    "# - the total number of training epochs at the end of each episode\n",
    "# - the total number of steps simulated at the end of each episode\n",
    "\n",
    "training_results.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a56a744",
   "metadata": {},
   "source": [
    "# Plot training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3258458",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Plot both the return per episode and the duration per episode during\n",
    "# training, together with their running average over 20 consecutive episodes\n",
    "\n",
    "N = 20 # number of episodes for running average\n",
    "\n",
    "def running_mean(x,N=20):\n",
    "        x_out = np.zeros(len(x)-N,dtype=float)\n",
    "        for i in range(len(x)-N):\n",
    "                x_out[i] = np.mean(x[i:i+N+1])\n",
    "        return x_out\n",
    "        \n",
    "def plot_returns_and_durations(training_results,filename=None):\n",
    "    fig,axes = plt.subplots(2,1,figsize=(5,8))\n",
    "    fig.subplots_adjust(hspace=0.0001)\n",
    "    #\n",
    "    # return as a function of episode\n",
    "    ax = axes[0]\n",
    "    x = training_results['epsiode_returns']\n",
    "    t = np.arange(len(x)) + 1\n",
    "    #\n",
    "    ax.plot(t,x,label='training',color='dodgerblue',)\n",
    "    # add running mean\n",
    "    x = running_mean(x=x,N=N)\n",
    "    t = np.arange(len(x)) + N\n",
    "    ax.plot(t,x,color='black',label='running mean')\n",
    "    #\n",
    "    ax.axhline(230,ls='--',\n",
    "               label='230',\n",
    "                        color='red')\n",
    "    #\n",
    "    ax.set_ylim(-499,350)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0,len(t)+100)\n",
    "    ax.set_xlabel(r'episode')\n",
    "    ax.set_ylabel(r'return')\n",
    "    #\n",
    "    #\n",
    "    ax = axes[1]\n",
    "    x = training_results['episode_durations']\n",
    "    t = np.arange(len(x)) + 1\n",
    "    #\n",
    "    ax.plot(t,x,label='training',color='dodgerblue',)\n",
    "    # add running mean\n",
    "    x = running_mean(x=x,N=N)\n",
    "    t = np.arange(len(x)) + N\n",
    "    ax.plot(t,x,color='black',label='running mean')\n",
    "    #\n",
    "    ax.axhline(1200,ls='--', # draw line outside of plot scale, \n",
    "                label='230', # to get the red dotted line into the legend\n",
    "                        color='red')\n",
    "    #\n",
    "    ax.set_ylim(0,1100)\n",
    "    ax.set_xlim(0,len(t)+100)\n",
    "    ax.set_xlabel(r'episode')\n",
    "    ax.set_ylabel(r'duration')\n",
    "    ax.legend(loc='upper right',bbox_to_anchor=(1.,1.35),\n",
    "                                framealpha=0.95,\n",
    "                        fontsize=18)\n",
    "    #\n",
    "    plt.show()\n",
    "    if filename != None:\n",
    "        fig.savefig(filename,bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_returns_and_durations(training_results=training_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "900a1245",
   "metadata": {},
   "source": [
    "# Create gameplay video using trained agent\n",
    "\n",
    "First we create a \"live\" video that pops up and shows Lunar Lander gameplay performed by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6140542",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# There is the issue that the game window freezes when running gym games \n",
    "# in jupyter notebooks, see https://github.com/openai/gym/issues/2433\n",
    "# We here use the fix from that website, which is to use the following\n",
    "# wrapper class:\n",
    "class PyGameWrapper(gym.Wrapper):\n",
    "    def render(self, **kwargs):\n",
    "        retval = self.env.render( **kwargs)\n",
    "        for event in pygame.event.get():\n",
    "            pass\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9e4b2",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Create a wrapped environment\n",
    "env = PyGameWrapper(gym.make('LunarLander-v2',render_mode='human'))\n",
    "\n",
    "N_episodes = 20\n",
    "\n",
    "result_string = 'Run {0}: duration = {1}, total return = {2:7.3f}'\n",
    "\n",
    "for j in range(N_episodes):\n",
    "    state, info = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    for i in itertools.count():\n",
    "        #env.render()\n",
    "\n",
    "        action = my_agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(result_string.format(j+1,i+1,total_reward))\n",
    "            break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09d4b344",
   "metadata": {},
   "source": [
    "We also create a video file containing 20 games played by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd02f7",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers.monitoring import video_recorder\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "video = video_recorder.VideoRecorder(env, './video.mp4'.format())\n",
    "\n",
    "N_episodes = 20\n",
    "\n",
    "result_string = 'Run {0}: duration = {1}, total return = {2:7.3f}'\n",
    "\n",
    "for j in range(N_episodes):\n",
    "    state, info = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    for i in itertools.count():\n",
    "        video.capture_frame()\n",
    "\n",
    "        action = my_agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(result_string.format(j+1,i+1,total_reward))\n",
    "            break\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d09cb",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d116e2643d320c245b477f305ead3df20562f29da6b60096a48b2443ed17af37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
